{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle detection using Machine Learning\n",
    "\n",
    "I've made a python notebook to offer detailed explanations about the process. Much of these explanations are directly taken from Udacity's Self Driving Car Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skimage.feature import hog\n",
    "from sklearn.externals import joblib\n",
    "from scipy import ndimage as ndi\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "from moviepy.editor import VideoFileClip\n",
    "from collections import deque\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img, conv='RGB2YCrCb'):\n",
    "    if conv == 'RGB2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    if conv == 'BGR2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    if conv == 'RGB2LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "    if conv == 'HLS':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2HLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Binning\n",
    "![Spatial Binning](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585c6ac5_spatial-binning/spatial-binning.jpg)\n",
    "\n",
    "Template matching is not a particularly robust method for finding vehicles unless you know exactly what your target object looks like. However, raw pixel values are still quite useful to include in your feature vector in searching for cars.\n",
    "\n",
    "While it could be cumbersome to include three color channels of a full resolution image, you can perform spatial binning on an image and still retain enough information to help in finding vehicles.\n",
    "\n",
    "As you can see in the example above, even going all the way down to 32 x 32 pixel resolution, the car itself is still clearly identifiable by eye, and this means that the relevant features are still preserved at this resolution.\n",
    "\n",
    "A convenient function for scaling down the resolution of an image is OpenCV's cv2.resize(). You can use it to scale a color image or a single color channel like this (you can find the original image here):\n",
    "\n",
    "```\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "image = mpimg.imread('test_img.jpg')\n",
    "small_img = cv2.resize(image, (32, 32))\n",
    "print(small_img.shape)\n",
    "(32, 32, 3)\n",
    "```\n",
    "\n",
    "If you then wanted to convert this to a one dimensional [feature](https://en.wikipedia.org/wiki/Feature_vector) vector, you could simply say something like:\n",
    "```\n",
    "feature_vec = small_img.ravel()\n",
    "print(feature_vec.shape)\n",
    "(3072,)\n",
    "```\n",
    "\n",
    "And the output would look someting like this - \n",
    "\n",
    "![Output](https://lh3.googleusercontent.com/qySCZslEqbnMoUXF0BU0NurTQl9TJqw6AsxG0TZQRQ5kbB_9OkAZ7Y3-1sIAX-LQurzT5bLJDuzmA54IT8A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Convert image to new color space (if specified)\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    # Return the feature vector\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Histograms:\n",
    "\n",
    "An image template is useful for detecting things that do not vary much in their appearance - for example, icons of emojis.\n",
    "But for most real world objects that do appear in different forms, orientation, and sizes, this technique does not\n",
    "work quite well. In template matching, you depend on raw color values laid out in a specific order, and that can vary a lot.\n",
    "So you need to find some transformations that are robust to changes in appearance. One such transform is to compute\n",
    "a histogram of color values for an image.\n",
    "\n",
    "When you compare the histogram of a known object with the regions of a test image, locations with a similar color distribution will reveal a close match. So we are no longer sensitive to a perfect arrangement of pixels. So objects that appear in slightly different orientations and sizes will still be a match.\n",
    "\n",
    "You can construct histograms of the R, G, and B channels like this:\n",
    "\n",
    "```\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "# Read in the image\n",
    "image = mpimg.imread('cutout1.jpg')\n",
    "\n",
    "# Take histograms in R, G, and B\n",
    "rhist = np.histogram(image[:,:,0], bins=32, range=(0, 256))\n",
    "ghist = np.histogram(image[:,:,1], bins=32, range=(0, 256))\n",
    "bhist = np.histogram(image[:,:,2], bins=32, range=(0, 256))\n",
    "```\n",
    "\n",
    "With np.histogram(), you don't actually have to specify the number of bins or the range, but here I've arbitrarily chosen 32 bins and specified range=(0, 256) in order to get orderly bin sizes. np.histogram() returns a tuple of two arrays. In this case, for example, rhist[0] contains the counts in each of the bins and rhist[1] contains the bin edges (so it is one element longer than rhist[0]).\n",
    "\n",
    "To look at a plot of these results, we can compute the bin centers from the bin edges. Each of the histograms in this case have the same bins, so we can just use the rhist bin edges:\n",
    "\n",
    "```\n",
    "# Generating bin centers\n",
    "bin_edges = rhist[1]\n",
    "bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "```\n",
    "And then summing up the results in a bar chart: \n",
    "```\n",
    "# Plot a figure with all three bar charts\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.bar(bin_centers, rhist[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('R Histogram')\n",
    "plt.subplot(132)\n",
    "plt.bar(bin_centers, ghist[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('G Histogram')\n",
    "plt.subplot(133)\n",
    "plt.bar(bin_centers, bhist[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('B Histogram')\n",
    "```\n",
    "\n",
    "The output should look like this:\n",
    "\n",
    "![Output](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585b1c43_rgb-histogram-plot/rgb-histogram-plot.jpg)\n",
    "\n",
    "These, collectively, are now our feature vector for this particular cutout image. We can concatenate them in the following way:\n",
    "\n",
    "```hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Gradients (HoG)\n",
    "\n",
    "Read more about it here - https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "\n",
    "The scikit-image package has a built in function to extract Histogram of Oriented Gradient features. The documentation for this function can be found [here](http://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=feature%20hog#skimage.feature.hog) and a brief explanation of the algorithm and tutorial can be found [here](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html).\n",
    "\n",
    "The scikit-image *hog()* function takes in a single color channel or grayscaled image as input, as well as various parameters. These parameters include orientations, pixels_per_cell and cells_per_block.\n",
    "\n",
    "The *number of orientations* is specified as an integer, and represents the number of orientation bins that the gradient information will be split up into in the histogram. Typical values are between 6 and 12 bins.\n",
    "\n",
    "The *pixels_per_cell* parameter specifies the cell size over which each gradient histogram is computed. This paramater is passed as a 2-tuple so you could have different cell sizes in x and y, but cells are commonly chosen to be square.\n",
    "\n",
    "The *cells_per_block* parameter is also passed as a 2-tuple, and specifies the local area over which the histogram counts in a given cell will be normalized. Block normalization is not necessarily required, but generally leads to a more robust feature set.\n",
    "\n",
    "There is another optional power law or \"gamma\" normalization scheme set by the flag transform_sqrt. This type of normalization may help reduce the effects of shadows or other illumination variation, but will cause an error if your image contains negative values (because it's taking the square root of image values).\n",
    "\n",
    "![HOG](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585b6011_hog-visualization/hog-visualization.jpg)\n",
    "\n",
    "This is where things get a little confusing though. Let's say you are computing HOG features for an image like the one shown above that is 64×64 pixels. If you set pixels_per_cell=(8, 8) and cells_per_block=(2, 2) and orientations=9. How many elements will you have in your HOG feature vector for the entire image?\n",
    "\n",
    "You might guess the number of orientations times the number of cells, or 9×8×8=576, but that's not the case if you're using block normalization! In fact, the HOG features for all cells in each block are computed at each block position and the block steps across and down through the image cell by cell.\n",
    "\n",
    "So, the actual number of features in your final feature vector will be the total number of block positions multiplied by the number of cells per block, times the number of orientations, or in the case shown above: 7×7×2×2×9=1764.\n",
    "For the example above, you would call the hog() function on a single color channel img like this:\n",
    "\n",
    "```\n",
    "from skimage.feature import hog\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "orient = 9\n",
    "\n",
    "features, hog_image = hog(img, orientations=orient,\n",
    "                          pixels_per_cell=(pix_per_cell, pix_per_cell), \n",
    "                          cells_per_block=(cell_per_block, cell_per_block), \n",
    "                          visualise=True, feature_vector=False,\n",
    "                          block_norm=\"L2-Hys\")\n",
    "```\n",
    "The visualise=True flag tells the function to output a visualization of the HOG feature computation as well, which we're calling hog_image in this case. If we take a look at a single color channel for a random car image, and its corresponding HOG visulization, they look like this:\n",
    "\n",
    "![HOG](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585c12d2_car-and-hog/car-and-hog.jpg)\n",
    "\n",
    "The HOG visualization is not actually the feature vector, but rather, a representation that shows the dominant gradient direction within each cell with brightness corresponding to the strength of gradients in that cell, much like the \"star\" representation in the last video.\n",
    "\n",
    "If you look at the features output, you'll find it's an array of shape 7×7×2×2×9. This corresponds to the fact that a grid of 7×7 blocks were sampled, with 2×2 cells in each block and 9 orientations per cell. You can unroll this array into a feature vector using features.ravel(), which yields, in this case, a one dimensional array of length 1764.\n",
    "\n",
    "Alternatively, you can set the feature_vector=True flag when calling the hog() function to automatically unroll the features. In the project, it could be useful to have a function defined that you could pass an image to with specifications for orientations, pixels_per_cell, and cells_per_block, as well as flags set for whether or not you want the feature vector unrolled and/or a visualization image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', orient=9,\n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # normalize the pixels.\n",
    "        #image = image.astype(np.float32)/255\n",
    "        # apply color conversion.\n",
    "        feature_image = convert_color(image, cspace)\n",
    "\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        file_features.append(spatial_features)\n",
    "        # Apply color_hist() also with a color space option now\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins, bins_range=hist_range)\n",
    "        file_features.append(hist_features)\n",
    "        # Append the new feature vector to the features list\n",
    "\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel],\n",
    "                                    orient, pix_per_cell, cell_per_block,\n",
    "                                    vis=False, feature_vec=True))\n",
    "            hog_features = np.ravel(hog_features)\n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient,\n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        file_features.append(hog_features)\n",
    "        # Append the new feature vector to the features list.\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window Search\n",
    "\n",
    "How many windows?\n",
    "![Car1](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585d79e7_car-identified/car-identified.jpg)\n",
    "\n",
    "To implement a sliding window search, you need to decide what size window you want to search, where in the image you want to start and stop your search, and how much you want windows to overlap. So, let's try an example to see how many windows we would be searching given a particular image size, window size, and overlap.\n",
    "\n",
    "Suppose you have an image that is 256 x 256 pixels and you want to search windows of a size 128 x 128 pixels each with an overlap of 50% between adjacent windows in both the vertical and horizontal dimensions. Your sliding window search would then look like this:\n",
    "\n",
    "![Cars](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585d80c2_sliding-window/sliding-window.jpg)\n",
    "\n",
    "The goal here is to write a function that takes in an image, start and stop positions in both x and y (imagine a bounding box for the entire search region), window size (x and y dimensions), and overlap fraction (also for both x and y). The function should return a list of bounding boxes for the search windows, which will then be passed to draw draw_boxes() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "        \n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    \n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    \n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    \n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step)\n",
    "    \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    \n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we define two new functions: *single_img_features()* and *search_windows()*. We can use these to search over all the windows defined by your slide_windows(), extract features at each window position, and predict with our classifier on each set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: \n",
    "        feature_image = np.copy(img) \n",
    "        \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        \n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "        \n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        \n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "        \n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB',\n",
    "                    spatial_size=(16, 16), hist_bins=32,\n",
    "                    hist_range=(0, 256), orient=9,\n",
    "                    pix_per_cell=8, cell_per_block=2,\n",
    "                    hog_channel=0, spatial_feat=True,\n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))\n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=colorspace,\n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                            orient=orient, pix_per_cell=pix_per_cell,\n",
    "                            cell_per_block=cell_per_block,\n",
    "                            hog_channel=hog_channel, spatial_feat=True,\n",
    "                            hist_feat=True, hog_feat=True)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        print(features.shape)\n",
    "        features = np.array(features)\n",
    "        X = np.array(features).reshape(1, -1)\n",
    "        test_features = scaler.transform(X)\n",
    "\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert windows to heatmap numpy array.\n",
    "def create_heatmap(windows, image_shape):\n",
    "    background = np.zeros(image_shape[:2])\n",
    "    for window in windows:\n",
    "        background[window[0][1]:window[1][1], window[0][0]:window[1][0]] += 1\n",
    "    return background\n",
    "\n",
    "# find the nonzero areas from a heatmap and\n",
    "# turn them to windows\n",
    "def find_windows_from_heatmap(image):\n",
    "    hot_windows = []\n",
    "    # Threshold the heatmap\n",
    "    thres = 0\n",
    "    image[image <= thres] = 0\n",
    "    # Set labels\n",
    "    labels = ndi.label(image)\n",
    "    # iterate through labels and find windows\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        hot_windows.append(bbox)\n",
    "    return hot_windows\n",
    "\n",
    "def combine_boxes(windows, image_shape):\n",
    "    hot_windows = []\n",
    "    image = None\n",
    "    if len(windows)>0:\n",
    "        # Create heatmap with windows\n",
    "        image = create_heatmap(windows, image_shape)\n",
    "        # find boxes from heatmap\n",
    "        hot_windows = find_windows_from_heatmap(image)\n",
    "    # return new windows\n",
    "    return hot_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide up into cars and notcars\n",
    "car_images = glob.glob('./images/vehicles/vehicles/*/*png')\n",
    "non_car_images = glob.glob('./images/non-vehicles/non-vehicles/*/*png')\n",
    "cars = []\n",
    "notcars = []\n",
    "for image in car_images:\n",
    "    cars.append(image)\n",
    "\n",
    "for image in non_car_images:\n",
    "    notcars.append(image)\n",
    "\n",
    "\n",
    "colorspace = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 8\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16)\n",
    "hist_bins = 32\n",
    "hist_range=(0, 256)\n",
    "\n",
    "#DO REMEMBER TO CHANGE THIS TO TRUE WHILE TRAINING, AND BACK TO FALSE AFTER TRAINING!!\n",
    "train_model = False\n",
    "#HLS, 4, 8, 95+\n",
    "#YCrCb, 4, 8, 95+\n",
    "filename_train = './classifier.joblib.pkl'\n",
    "filename_scaler = './scaler.joblib.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Normalize Features\n",
    "\n",
    "![Image](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/585c6731_scaled-features-vis/scaled-features-vis.jpg)\n",
    "Now that we've got several feature extraction methods in your toolkit, we're almost ready to train a classifier, but first, as in any machine learning application, we need to normalize your data. Python's sklearn package provides you with the StandardScaler() method to accomplish this task. To read more about how you can choose different normalizations with the StandardScaler() method, check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "To apply StandardScaler() we need to first have your data in the right format, as a numpy array where each row is a single feature vector. We can create a list of feature vectors, and then convert them like this:\n",
    "```\n",
    "import numpy as np\n",
    "feature_list = [feature_vec1, feature_vec2, ...]\n",
    "# Create an array stack, NOTE: StandardScaler() expects np.float64\n",
    "X = np.vstack(feature_list).astype(np.float64)\n",
    "```\n",
    "You can then fit a scaler to X, and scale it like this:\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "```\n",
    "Now, scaled_X contains the normalized feature vectors.\n",
    "\n",
    "Now we gotta write a function that takes in a list of image filenames, then reads them one by one, then applies a color conversion (if necessary) and uses bin_spatial() and color_hist() to generate feature vectors. The function should then concatenate those two feature vectors and append the result to a list. After cycling through all the images, the function should return the list of feature vectors. Something like this:\n",
    "```\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "        # Read in each one by one\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        # Apply bin_spatial() to get spatial color features\n",
    "        # Apply color_hist() to get color histogram features\n",
    "        # Append the new feature vector to the features list\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Hyperparameters\n",
    "We can optimize the Gamma and C parameters for an SVC classifier.\n",
    "\n",
    "Successfully tuning your algorithm involves searching for a kernel, a gamma value and a C value that minimize prediction error. To tune your SVM vehicle detection model, you can use one of scikit-learn's parameter tuning algorithms.\n",
    "\n",
    "When tuning SVM, remember that you can only tune the C parameter with a linear kernel. For a non-linear kernel, you can tune C and gamma.\n",
    "\n",
    "#### Parameter Tuning in Scikit-learn\n",
    "Scikit-learn includes two algorithms for carrying out an automatic parameter search:\n",
    "\n",
    "* [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "* [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)\n",
    "\n",
    "GridSearchCV exhaustively works through multiple parameter combinations, cross-validating as it goes. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
    "\n",
    "For example, if I input the values C:[0.1, 1, 10] and gamma:[0.1, 1, 10], gridSearchCV will train and cross-validate every possible combination of (C, gamma): (0.1, 0.1), (0.1, 1), (0.1, 10), (1, .1), (1, 1), etc.\n",
    "\n",
    "RandomizedSearchCV works similarly to GridSearchCV except RandomizedSearchCV takes a random sample of parameter combinations. RandomizedSearchCV is faster than GridSearchCV since RandomizedSearchCV uses a subset of the parameter combinations.\n",
    "\n",
    "#### Cross-validation with GridSearchCV\n",
    "GridSearchCV uses 3-fold cross validation to determine the best performing parameter set. GridSearchCV will take in a training set and divide the training set into three equal partitions. The algorithm will train on two partitions and then validate using the third partition. Then GridSearchCV chooses a different partition for validation and trains with the other two partitions. Finally, GridSearchCV uses the last remaining partition for cross-validation and trains with the other two partitions.\n",
    "\n",
    "By default, GridSearchCV uses accuracy as an error metric by averaging the accuracy for each partition. So for every possible parameter combination, GridSearchCV calculates an accuracy score. Then GridSearchCV will choose the parameter combination that performed the best.\n",
    "\n",
    "scikit-learn Cross Validation Example\n",
    "Here's an example from the sklearn documentation for implementing GridSearchCV:\n",
    "```\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "Let's break this down line by line.\n",
    "\n",
    "```\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "``` \n",
    "\n",
    "A dictionary of the parameters, and the possible values they may take. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10).\n",
    "\n",
    "Then a 'grid' of all the following combinations of values for (kernel, C) are automatically generated:\n",
    "\n",
    "('rbf', 1)\t('rbf', 10)\n",
    "('linear', 1)\t('linear', 10)\n",
    "\n",
    "Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
    "\n",
    "```\n",
    "svr = svm.SVC() \n",
    "```\n",
    "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line.\n",
    "\n",
    "```\n",
    "clf = grid_search.GridSearchCV(svr, parameters) \n",
    "```\n",
    "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try.\n",
    "\n",
    "```\n",
    "clf.fit(iris.data, iris.target) \n",
    "```\n",
    "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. You can now access the parameter values via clf.best_params_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for GridSearchCV\n",
    "#grid_search_parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.001, 0.01, 0.1, 1, 10], 'gamma':  [0.001, 0.01, 0.1, 1]}\n",
    "if train_model:\n",
    "    t=time.time()\n",
    "    car_features = extract_features(cars, cspace=colorspace, orient=orient,\n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel)\n",
    "    notcar_features = extract_features(notcars, cspace=colorspace, orient=orient,\n",
    "                            pix_per_cell=pix_per_cell, cell_per_block=cell_per_block,\n",
    "                            hog_channel=hog_channel)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to extract HOG features...')\n",
    "\n",
    "    print(\"car feature shape: \", len(car_features))\n",
    "    print(\"non-car feature shape: \", len(notcar_features))\n",
    "        # Create an array stack of feature vectors\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)\n",
    "    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    \n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    \n",
    "    \n",
    "    # Split up data into randomized training and test sets\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "    # Use a linear SVC\n",
    "    clf = svm.SVC(kernel='linear', C=0.001, gamma=0.001)\n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    #clf = GridSearchCV(svc, grid_search_parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "    # Check the score of the SVC\n",
    "    print('Test Accuracy of SVC = ', round(clf.score(X_test, y_test), 4))\n",
    "    # Check the prediction time for a single sample\n",
    "    t=time.time()\n",
    "    n_predict = 10\n",
    "    print('My SVC predicts: ', clf.predict(X_test[0:n_predict]))\n",
    "    print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n",
    "\n",
    "        # save the trained model\n",
    "    _ = joblib.dump(clf, filename_train, compress=9)\n",
    "    _ = joblib.dump(X_scaler, filename_scaler, compress=9)\n",
    "else:\n",
    "    # load the trained model\n",
    "    clf = joblib.load(filename_train)\n",
    "    X_scaler = joblib.load(filename_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    Pipeline to detect and track vehicles across images of video frames\n",
    "    \"\"\"\n",
    "    draw_image = np.copy(image)\n",
    "\n",
    "    windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 640],\n",
    "                    xy_window=(96, 96), xy_overlap=(0.75, 0.75))\n",
    "\n",
    "    windows += slide_window(image, x_start_stop=[32, None], y_start_stop=[400, 610],\n",
    "                    xy_window=(144, 144), xy_overlap=(0.75, 0.75))\n",
    "    windows += slide_window(image, x_start_stop=[410, 1280], y_start_stop=[390, 540],\n",
    "                    xy_window=(192, 192), xy_overlap=(0.75, 0.75))\n",
    "\n",
    "    hot_windows = search_windows(image, windows, clf, X_scaler, color_space=colorspace,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=True,\n",
    "                        hist_feat=True, hog_feat=True)\n",
    "\n",
    "\n",
    "    #draw_image = draw_boxes(draw_image, hot_windows, color=(255, 0, 0), thick=6)\n",
    "    combined_windows = combine_boxes(hot_windows, image.shape)\n",
    "    filtered_windows = []\n",
    "    # no car detection yet, create new detections and add them to the list.\n",
    "    if len(detections) == 0:\n",
    "        for window in combined_windows:\n",
    "            box_points = get_box_points(window)\n",
    "            new_car = Detection()\n",
    "            new_car.add(box_points)\n",
    "            detections.append(new_car)\n",
    "            window_img = draw_boxes(draw_image, filtered_windows, color=(0, 0, 255), thick=6)\n",
    "            return window_img\n",
    "    else:\n",
    "        boxes_copy = copy.copy(combined_windows)\n",
    "        # Run thorugh all the existing detections and see if any new detections\n",
    "        # matche with them.\n",
    "        # if match is found add to the detection.\n",
    "        # If not found decrease the confidence of the previous detection.\n",
    "        non_detected_cars_idxs = []\n",
    "        for car_idx, car in enumerate(detections):\n",
    "            match_found = False\n",
    "            box_detection_idx = 0\n",
    "            for idx, box in enumerate(boxes_copy):\n",
    "                box_points = get_box_points(box)\n",
    "                if car.match_detection(box_points):\n",
    "                    match_found = True\n",
    "                    if car.consecutive_detection >= min_consecutive_detection:\n",
    "                        average_box = car.average_detections()\n",
    "                        filtered_windows.append(((average_box[0],average_box[1]),(average_box[2], average_box[3])))\n",
    "\n",
    "\t\t    # remove after the match.\n",
    "                    box_detection_idx = idx\n",
    "                    # Match for the car is found, break the inner loop\n",
    "                    break\n",
    "\n",
    "            # Match not found for the previous detection, decrease its confidence.\n",
    "            # The delete detections is true, remove the detection from the list of previous detections.\n",
    "            if not match_found:\n",
    "                delete_Detection = car.failed_detect()\n",
    "                if delete_Detection:\n",
    "                    non_detected_cars_idxs.append(car_idx)\n",
    "                else:\n",
    "                    average_box = car.average_detections()\n",
    "                    filtered_windows.append(((average_box[0],average_box[1]),(average_box[2], average_box[3])))\n",
    "            else:\n",
    "                # Delete the detected box from the list of boxes to be mathched.\n",
    "                del boxes_copy[box_detection_idx]\n",
    "\n",
    "        # Remove all the undetected cars from the list of detections using thier saved index.\n",
    "        if len(non_detected_cars_idxs) > 0:\n",
    "             non_detected_cars_idxs =  non_detected_cars_idxs[::-1]\n",
    "             for i in non_detected_cars_idxs:\n",
    "                del detections[i]\n",
    "\n",
    "        # Add the unmatched boxes to the detections array.\n",
    "        for box in boxes_copy:\n",
    "            box_points = get_box_points(box)\n",
    "            new_car = Detection()\n",
    "            new_car.add(box_points)\n",
    "            detections.append(new_car)\n",
    "\n",
    "            # If the match is not found decrease the confidence of the detection.\n",
    "\n",
    "\n",
    "\n",
    "    window_img = draw_boxes(draw_image, filtered_windows, color=(0, 0, 255), thick=6)\n",
    "\n",
    "    return window_img\n",
    "\n",
    "def get_box_points(box):\n",
    "    \"\"\"\n",
    "    Takes in box points of form ((x1,y1), (x2, y2)) and converts it to form\n",
    "    [x1, y1, x2, y2].\n",
    "    \"\"\"\n",
    "    box_points = []\n",
    "    x1, y1 = box[0]\n",
    "    x2, y2 = box[1]\n",
    "\n",
    "    box_points.append(x1)\n",
    "    box_points.append(y1)\n",
    "    box_points.append(x2)\n",
    "    box_points.append(y2)\n",
    "    return box_points\n",
    "\n",
    "\n",
    "margin = 100\n",
    "min_consecutive_detection = 8\n",
    "max_allowed_miss = 4\n",
    "confidence_thresh = 10\n",
    "\n",
    "def is_within_margin(a, b):\n",
    "    if abs(a-b) > margin:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class Detection():\n",
    "    def __init__(self):\n",
    "        # the box coordinates in the form [x1,y1,x2,y2]\n",
    "        self.last_box = []\n",
    "        # number of consecutive frames in which the car has been detected.\n",
    "        self.consecutive_detection = 0\n",
    "        # number of consecutive frames in which the car has not been found.\n",
    "        self.consecutive_miss = 0\n",
    "        # the box coordinates of last n detections in the form deque([[x1, y1, x2, y2], [x1, y1, x2, y2], [x1, y1, x2, y2]...], maxlen=5)\n",
    "        self.last_n_detections = deque(maxlen=10)\n",
    "        # [avg x1 , avg y1, avg x2, avgy2] of last n detections.\n",
    "        self.average_box = []\n",
    "\n",
    "    def add(self, box):\n",
    "        \"\"\"\n",
    "        box argument should be of format [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        self.last_box = box\n",
    "        self.consecutive_detection =  self.consecutive_detection + 1\n",
    "        self.last_n_detections.append(box)\n",
    "        self.average_detections()\n",
    "        # set the previous count of consecutive misses to 0.\n",
    "        self.consecutive_miss = 0\n",
    "\n",
    "    def average_detections(self):\n",
    "        \"\"\"\n",
    "        Find the mean of detections in the deque.\n",
    "        \"\"\"\n",
    "\n",
    "        self.average_box = np.mean(self.last_n_detections, axis=0)\n",
    "        return self.average_box\n",
    "\n",
    "    def match_detection(self, box):\n",
    "        \"\"\"\n",
    "        Checks whether the box is very close/similar to the [x1, y1, x2, y2]\n",
    "        box argument should be of format [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        for point in box:\n",
    "            # see if all the points in the box lies within the margin of the last detection.\n",
    "\n",
    "            if not is_within_margin(point, self.last_box[i]):\n",
    "                return False\n",
    "            i = i + 1\n",
    "        # If the match found then add it to the detection.\n",
    "        self.add(box)\n",
    "        return True\n",
    "\n",
    "    def failed_detect(self):\n",
    "         delete_detection = True\n",
    "         self.consecutive_miss = self.consecutive_miss + 1\n",
    "         # In case the car doesn't get for more than 3 frames consecutively we discard the\n",
    "         # object.\n",
    "         if self.consecutive_miss  > max_allowed_miss:\n",
    "             return delete_detection\n",
    "        # This helps remove the stray false positives which doesn't get detected in\n",
    "        # consecutive frames.\n",
    "         if self.consecutive_detection < min_consecutive_detection:\n",
    "             return delete_detection\n",
    "\n",
    "\n",
    "         # Wait till you the miss becomes greater than max_allowed_miss.\n",
    "         return False\n",
    "\n",
    "\n",
    "# array of Detection class.\n",
    "detections = []\n",
    "# output video directory\n",
    "video_output = './video-tracking-output.mp4'\n",
    "# input video directory\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "# video process pipline\n",
    "#video_clip = clip1.fl_image(process_image)\n",
    "# write processed files\n",
    "#video_clip.write_videofile(video_output, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
